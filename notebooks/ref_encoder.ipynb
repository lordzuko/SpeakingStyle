{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a114d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db689a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/tc046/tc046/lordzuko/miniconda3/envs/fs2/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import numpy as np \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from text import text_to_sequence\n",
    "from utils.tools import pad_1D, pad_2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626dc4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    train_config = \"/work/tc046/tc046/lordzuko/work/SpeakingStyle/config/BC2013/train.yaml\"\n",
    "    model_config = \"/work/tc046/tc046/lordzuko/work/SpeakingStyle/config/BC2013/model.yaml\"\n",
    "    preprocess_config = \"/work/tc046/tc046/lordzuko/work/SpeakingStyle/config/BC2013/preprocess.yaml\"\n",
    "    restore_step = 0\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a8ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_config = yaml.load(\n",
    "        open(args.preprocess_config, \"r\"), Loader=yaml.FullLoader\n",
    "    )\n",
    "model_config = yaml.load(open(args.model_config, \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(args.train_config, \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "configs = preprocess_config, model_config, train_config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a5b810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset.py\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self, filename, preprocess_config, model_config, train_config, sort=False, drop_last=False\n",
    "    ):\n",
    "        self.dataset_name = preprocess_config[\"dataset\"]\n",
    "        self.preprocessed_path = preprocess_config[\"path\"][\"preprocessed_path\"]\n",
    "        self.cleaners = preprocess_config[\"preprocessing\"][\"text\"][\"text_cleaners\"]\n",
    "        self.batch_size = train_config[\"optimizer\"][\"batch_size\"]\n",
    "        self.load_spker_embed = model_config[\"multi_speaker\"] \\\n",
    "            and preprocess_config[\"preprocessing\"][\"speaker_embedder\"] != 'none'\n",
    "\n",
    "        self.basename, self.speaker, self.text, self.raw_text = self.process_meta(\n",
    "            filename\n",
    "        )\n",
    "        with open(os.path.join(self.preprocessed_path, \"speakers.json\")) as f:\n",
    "            self.speaker_map = json.load(f)\n",
    "        self.sort = sort\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        basename = self.basename[idx]\n",
    "        speaker = self.speaker[idx]\n",
    "        speaker_id = self.speaker_map[speaker]\n",
    "        raw_text = self.raw_text[idx]\n",
    "        phone = np.array(text_to_sequence(self.text[idx], self.cleaners))\n",
    "        mel_path = os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"mel\",\n",
    "            \"{}-mel-{}.npy\".format(speaker, basename),\n",
    "        )\n",
    "        mel = np.load(mel_path)\n",
    "        pitch_path = os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"pitch\",\n",
    "            \"{}-pitch-{}.npy\".format(speaker, basename),\n",
    "        )\n",
    "        pitch = np.load(pitch_path) # Phoneme Level\n",
    "        ref_pitch_path = os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"pitch_frame\",\n",
    "            \"{}-pitch-{}.npy\".format(speaker, basename),\n",
    "        )\n",
    "        ref_pitch = np.load(ref_pitch_path) # Frame Level\n",
    "        energy_path = os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"energy\",\n",
    "            \"{}-energy-{}.npy\".format(speaker, basename),\n",
    "        )\n",
    "        energy = np.load(energy_path) # Phoneme Level\n",
    "        ref_energy_path = os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"energy_frame\",\n",
    "            \"{}-energy-{}.npy\".format(speaker, basename),\n",
    "        )\n",
    "        ref_energy = np.load(ref_energy_path) # Frame Level\n",
    "        duration_path = os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"duration\",\n",
    "            \"{}-duration-{}.npy\".format(speaker, basename),\n",
    "        )\n",
    "        duration = np.load(duration_path)\n",
    "        spker_embed = np.load(os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"spker_embed\",\n",
    "            \"{}-spker_embed.npy\".format(speaker),\n",
    "        )) if self.load_spker_embed else None\n",
    "\n",
    "        sample = {\n",
    "            \"id\": basename,\n",
    "            \"speaker\": speaker_id,\n",
    "            \"text\": phone,\n",
    "            \"raw_text\": raw_text,\n",
    "            \"mel\": mel,\n",
    "            \"pitch\": pitch,\n",
    "            \"energy\": energy,\n",
    "            \"duration\": duration,\n",
    "            \"spker_embed\": spker_embed,\n",
    "            \"ref_pitch\": ref_pitch,\n",
    "            \"ref_energy\": ref_energy,\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def process_meta(self, filename):\n",
    "        with open(\n",
    "            os.path.join(self.preprocessed_path, filename), \"r\", encoding=\"utf-8\"\n",
    "        ) as f:\n",
    "            name = []\n",
    "            speaker = []\n",
    "            text = []\n",
    "            raw_text = []\n",
    "            for line in f.readlines():\n",
    "                n, s, t, r = line.strip(\"\\n\").split(\"|\")\n",
    "                name.append(n)\n",
    "                speaker.append(s)\n",
    "                text.append(t)\n",
    "                raw_text.append(r)\n",
    "            return name, speaker, text, raw_text\n",
    "\n",
    "    def reprocess(self, data, idxs):\n",
    "        ids = [data[idx][\"id\"] for idx in idxs]\n",
    "        speakers = [data[idx][\"speaker\"] for idx in idxs]\n",
    "        texts = [data[idx][\"text\"] for idx in idxs]\n",
    "        raw_texts = [data[idx][\"raw_text\"] for idx in idxs]\n",
    "        mels = [data[idx][\"mel\"] for idx in idxs]\n",
    "        pitches = [data[idx][\"pitch\"] for idx in idxs]\n",
    "        ref_pitches = [data[idx][\"ref_pitch\"] for idx in idxs]\n",
    "        energies = [data[idx][\"energy\"] for idx in idxs]\n",
    "        ref_energies = [data[idx][\"ref_energy\"] for idx in idxs]\n",
    "        durations = [data[idx][\"duration\"] for idx in idxs]\n",
    "        spker_embeds = np.concatenate(np.array([data[idx][\"spker_embed\"] for idx in idxs]), axis=0) \\\n",
    "            if self.load_spker_embed else None\n",
    "\n",
    "        text_lens = np.array([text.shape[0] for text in texts])\n",
    "        mel_lens = np.array([mel.shape[0] for mel in mels])\n",
    "\n",
    "        speakers = np.array(speakers)\n",
    "        texts = pad_1D(texts)\n",
    "        mels = pad_2D(mels)\n",
    "        pitches = pad_1D(pitches)\n",
    "        ref_pitches = pad_1D(ref_pitches)\n",
    "        energies = pad_1D(energies)\n",
    "        ref_energies = pad_1D(ref_energies)\n",
    "        durations = pad_1D(durations)\n",
    "\n",
    "        return (\n",
    "            ids,\n",
    "            raw_texts,\n",
    "            speakers,\n",
    "            texts,\n",
    "            text_lens,\n",
    "            max(text_lens),\n",
    "            mels,\n",
    "            mel_lens,\n",
    "            max(mel_lens),\n",
    "            pitches,\n",
    "            energies,\n",
    "            durations,\n",
    "            spker_embeds,\n",
    "            ref_pitches,\n",
    "            ref_energies,\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        data_size = len(data)\n",
    "\n",
    "        if self.sort:\n",
    "            len_arr = np.array([d[\"text\"].shape[0] for d in data])\n",
    "            idx_arr = np.argsort(-len_arr)\n",
    "        else:\n",
    "            idx_arr = np.arange(data_size)\n",
    "\n",
    "        tail = idx_arr[len(idx_arr) - (len(idx_arr) % self.batch_size) :]\n",
    "        idx_arr = idx_arr[: len(idx_arr) - (len(idx_arr) % self.batch_size)]\n",
    "        idx_arr = idx_arr.reshape((-1, self.batch_size)).tolist()\n",
    "        if not self.drop_last and len(tail) > 0:\n",
    "            idx_arr += [tail.tolist()]\n",
    "\n",
    "        output = list()\n",
    "        for idx in idx_arr:\n",
    "            output.append(self.reprocess(data, idx))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, filepath, preprocess_config, model_config):\n",
    "        self.cleaners = preprocess_config[\"preprocessing\"][\"text\"][\"text_cleaners\"]\n",
    "        self.preprocessed_path = preprocess_config[\"path\"][\"preprocessed_path\"]\n",
    "        self.load_spker_embed = model_config[\"multi_speaker\"] \\\n",
    "            and preprocess_config[\"preprocessing\"][\"speaker_embedder\"] != 'none'\n",
    "\n",
    "        self.basename, self.speaker, self.text, self.raw_text = self.process_meta(\n",
    "            filepath\n",
    "        )\n",
    "        with open(\n",
    "            os.path.join(\n",
    "                preprocess_config[\"path\"][\"preprocessed_path\"], \"speakers.json\"\n",
    "            )\n",
    "        ) as f:\n",
    "            self.speaker_map = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        basename = self.basename[idx]\n",
    "        speaker = self.speaker[idx]\n",
    "        speaker_id = self.speaker_map[speaker]\n",
    "        raw_text = self.raw_text[idx]\n",
    "        phone = np.array(text_to_sequence(self.text[idx], self.cleaners))\n",
    "        mel_path = os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"mel\",\n",
    "            \"{}-mel-{}.npy\".format(speaker, basename),\n",
    "        )\n",
    "        mel = np.load(mel_path)\n",
    "        ref_pitch_path = os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"pitch_frame\",\n",
    "            \"{}-pitch-{}.npy\".format(speaker, basename),\n",
    "        )\n",
    "        ref_pitch = np.load(ref_pitch_path) # Frame Level\n",
    "        ref_energy_path = os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"energy_frame\",\n",
    "            \"{}-energy-{}.npy\".format(speaker, basename),\n",
    "        )\n",
    "        ref_energy = np.load(ref_energy_path) # Frame Level\n",
    "        spker_embed = np.load(os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"spker_embed\",\n",
    "            \"{}-spker_embed.npy\".format(speaker),\n",
    "        )) if self.load_spker_embed else None\n",
    "\n",
    "        return (basename, speaker_id, phone, raw_text, mel, spker_embed, ref_pitch, ref_energy)\n",
    "\n",
    "    def process_meta(self, filename):\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            name = []\n",
    "            speaker = []\n",
    "            text = []\n",
    "            raw_text = []\n",
    "            for line in f.readlines():\n",
    "                n, s, t, r = line.strip(\"\\n\").split(\"|\")\n",
    "                name.append(n)\n",
    "                speaker.append(s)\n",
    "                text.append(t)\n",
    "                raw_text.append(r)\n",
    "            return name, speaker, text, raw_text\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        ids = [d[0] for d in data]\n",
    "        speakers = np.array([d[1] for d in data])\n",
    "        texts = [d[2] for d in data]\n",
    "        raw_texts = [d[3] for d in data]\n",
    "        mels = [d[4] for d in data]\n",
    "        spker_embeds = np.concatenate(np.array([d[5] for d in data]), axis=0) \\\n",
    "            if self.load_spker_embed else None\n",
    "        ref_pitches = [d[6] for d in data]\n",
    "        ref_energies = [d[7] for d in data]\n",
    "\n",
    "        text_lens = np.array([text.shape[0] for text in texts])\n",
    "        mel_lens = np.array([mel.shape[0] for mel in mels])\n",
    "\n",
    "        texts = pad_1D(texts)\n",
    "        mels = pad_2D(mels)\n",
    "        ref_pitches = pad_1D(ref_pitches)\n",
    "        ref_energies = pad_1D(ref_energies)\n",
    "\n",
    "        return (\n",
    "            ids,\n",
    "            raw_texts,\n",
    "            speakers,\n",
    "            texts,\n",
    "            text_lens,\n",
    "            max(text_lens),\n",
    "            mels,\n",
    "            mel_lens,\n",
    "            max(mel_lens),\n",
    "            spker_embeds,\n",
    "            ref_pitches,\n",
    "            ref_energies,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca20635",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.model import get_vocoder, get_param_num, get_named_param, get_model\n",
    "from utils.tools import get_configs_of, to_device, log, synth_one_sample\n",
    "# from model import FastSpeech2Loss\n",
    "\n",
    "# Get dataset\n",
    "dataset = Dataset(\n",
    "    \"train.txt\", preprocess_config, model_config, train_config, sort=True, drop_last=True\n",
    ")\n",
    "batch_size = train_config[\"optimizer\"][\"batch_size\"]\n",
    "group_size = 4  # Set this larger than 1 to enable sorting in Dataset\n",
    "assert batch_size * group_size < len(dataset)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size * group_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    num_workers=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ded2c1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of FastSpeech2 Parameters: 50477403\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "model, optimizer = get_model(\n",
    "    args, configs, device, train=True, ignore_layers=train_config[\"ignore_layers\"])\n",
    "model = nn.DataParallel(model)\n",
    "num_param = get_param_num(model)\n",
    "# Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)\n",
    "print(\"Number of FastSpeech2 Parameters:\", num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c63e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "# Load vocoder\n",
    "vocoder = get_vocoder(model_config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd56c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batchs in loader:\n",
    "    for batch in batchs:\n",
    "        batch = to_device(batch, device)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef5e351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n"
     ]
    }
   ],
   "source": [
    "output = model(*(batch[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batchs in loader:\n",
    "#     for batch in batchs:\n",
    "#         batch = to_device(batch, device)\n",
    "\n",
    "#         # Forward\n",
    "#         output = model(*(batch[2:]))\n",
    "#         print(output)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb256b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FastSpeech2Loss(nn.Module):\n",
    "#     \"\"\" FastSpeech2 Loss \"\"\"\n",
    "\n",
    "#     def __init__(self, preprocess_config, model_config):\n",
    "#         super(FastSpeech2Loss, self).__init__()\n",
    "#         self.pitch_feature_level = preprocess_config[\"preprocessing\"][\"pitch\"][\n",
    "#             \"feature\"\n",
    "#         ]\n",
    "#         self.energy_feature_level = preprocess_config[\"preprocessing\"][\"energy\"][\n",
    "#             \"feature\"\n",
    "#         ]\n",
    "#         self.mse_loss = nn.MSELoss()\n",
    "#         self.mae_loss = nn.L1Loss()\n",
    "\n",
    "#     def forward(self, inputs, predictions):\n",
    "#         (\n",
    "#             mel_targets,\n",
    "#             _,\n",
    "#             _,\n",
    "#             pitch_targets,\n",
    "#             energy_targets,\n",
    "#             duration_targets,\n",
    "#         ) = inputs[6:]\n",
    "#         (\n",
    "#             mel_predictions,\n",
    "#             postnet_mel_predictions,\n",
    "#             pitch_predictions,\n",
    "#             energy_predictions,\n",
    "#             log_duration_predictions,\n",
    "#             _,\n",
    "#             src_masks,\n",
    "#             mel_masks,\n",
    "#             _,\n",
    "#             _,\n",
    "#         ) = predictions\n",
    "#         src_masks = ~src_masks\n",
    "#         mel_masks = ~mel_masks\n",
    "#         log_duration_targets = torch.log(duration_targets.float() + 1)\n",
    "#         mel_targets = mel_targets[:, : mel_masks.shape[1], :]\n",
    "#         mel_masks = mel_masks[:, :mel_masks.shape[1]]\n",
    "\n",
    "#         log_duration_targets.requires_grad = False\n",
    "#         pitch_targets.requires_grad = False\n",
    "#         energy_targets.requires_grad = False\n",
    "#         mel_targets.requires_grad = False\n",
    "\n",
    "#         if self.pitch_feature_level == \"phoneme_level\":\n",
    "#             pitch_predictions = pitch_predictions.masked_select(src_masks)\n",
    "#             pitch_targets = pitch_targets.masked_select(src_masks)\n",
    "#         elif self.pitch_feature_level == \"frame_level\":\n",
    "#             pitch_predictions = pitch_predictions.masked_select(mel_masks)\n",
    "#             pitch_targets = pitch_targets.masked_select(mel_masks)\n",
    "\n",
    "#         if self.energy_feature_level == \"phoneme_level\":\n",
    "#             energy_predictions = energy_predictions.masked_select(src_masks)\n",
    "#             energy_targets = energy_targets.masked_select(src_masks)\n",
    "#         if self.energy_feature_level == \"frame_level\":\n",
    "#             energy_predictions = energy_predictions.masked_select(mel_masks)\n",
    "#             energy_targets = energy_targets.masked_select(mel_masks)\n",
    "\n",
    "#         log_duration_predictions = log_duration_predictions.masked_select(src_masks)\n",
    "#         log_duration_targets = log_duration_targets.masked_select(src_masks)\n",
    "\n",
    "#         mel_predictions = mel_predictions.masked_select(mel_masks.unsqueeze(-1))\n",
    "#         postnet_mel_predictions = postnet_mel_predictions.masked_select(\n",
    "#             mel_masks.unsqueeze(-1)\n",
    "#         )\n",
    "#         mel_targets = mel_targets.masked_select(mel_masks.unsqueeze(-1))\n",
    "\n",
    "#         mel_loss = self.mae_loss(mel_predictions, mel_targets)\n",
    "#         postnet_mel_loss = self.mae_loss(postnet_mel_predictions, mel_targets)\n",
    "\n",
    "#         pitch_loss = self.mse_loss(pitch_predictions, pitch_targets)\n",
    "#         energy_loss = self.mse_loss(energy_predictions, energy_targets)\n",
    "#         duration_loss = self.mse_loss(log_duration_predictions, log_duration_targets)\n",
    "\n",
    "#         total_loss = (\n",
    "#             mel_loss + postnet_mel_loss + duration_loss + pitch_loss + energy_loss\n",
    "#         )\n",
    "\n",
    "#         return (\n",
    "#             total_loss,\n",
    "#             mel_loss,\n",
    "#             postnet_mel_loss,\n",
    "#             pitch_loss,\n",
    "#             energy_loss,\n",
    "#             duration_loss,\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce01066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Init logger\n",
    "# for p in train_config[\"path\"].values():\n",
    "#     os.makedirs(p, exist_ok=True)\n",
    "# train_log_path = os.path.join(train_config[\"path\"][\"log_path\"], \"train\")\n",
    "# val_log_path = os.path.join(train_config[\"path\"][\"log_path\"], \"val\")\n",
    "# os.makedirs(train_log_path, exist_ok=True)\n",
    "# os.makedirs(val_log_path, exist_ok=True)\n",
    "# train_logger = SummaryWriter(train_log_path)\n",
    "# val_logger = SummaryWriter(val_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3330475d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|          | 0/90000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# # Training\n",
    "# named_param = ['s_gamma', 's_beta']\n",
    "# step = args.restore_step + 1\n",
    "# epoch = 1\n",
    "# grad_acc_step = train_config[\"optimizer\"][\"grad_acc_step\"]\n",
    "# grad_clip_thresh = train_config[\"optimizer\"][\"grad_clip_thresh\"]\n",
    "# total_step = train_config[\"step\"][\"total_step\"]\n",
    "# log_step = train_config[\"step\"][\"log_step\"]\n",
    "# save_step = train_config[\"step\"][\"save_step\"]\n",
    "# synth_step = train_config[\"step\"][\"synth_step\"]\n",
    "# val_step = train_config[\"step\"][\"val_step\"]\n",
    "\n",
    "# outer_bar = tqdm(total=total_step, desc=\"Training\", position=0)\n",
    "# outer_bar.n = args.restore_step\n",
    "# outer_bar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6021acdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:   0%|          | 0/4931 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m(batch[\u001b[38;5;241m2\u001b[39m:]))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Cal Loss\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m losses \u001b[38;5;241m=\u001b[39m Loss(batch, output, step, \u001b[43mget_named_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamed_param\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m losses, lambdas \u001b[38;5;241m=\u001b[39m losses[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     13\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/mnt/lustre/indy2lfs/work/tc046/tc046/lordzuko/work/SpeakingStyle/utils/model.py:54\u001b[0m, in \u001b[0;36mget_named_param\u001b[0;34m(model, tags)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([tag \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags]):\n\u001b[1;32m     53\u001b[0m         params\u001b[38;5;241m.\u001b[39mappend(param)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "# while True:\n",
    "#     inner_bar = tqdm(total=len(loader), desc=\"Epoch {}\".format(epoch), position=1)\n",
    "#     for batchs in loader:\n",
    "#         for batch in batchs:\n",
    "#             batch = to_device(batch, device)\n",
    "\n",
    "#             # Forward\n",
    "#             output = model(*(batch[2:]))\n",
    "\n",
    "#             # Cal Loss\n",
    "#             losses = Loss(batch, output, step, get_named_param(model, named_param))\n",
    "#             losses, lambdas = losses[:-2], losses[-2:]\n",
    "#             total_loss = losses[0]\n",
    "\n",
    "#             # Backward\n",
    "#             total_loss = total_loss / grad_acc_step\n",
    "#             total_loss.backward()\n",
    "#             if step % grad_acc_step == 0:\n",
    "#                 # Clipping gradients to avoid gradient explosion\n",
    "#                 nn.utils.clip_grad_norm_(model.parameters(), grad_clip_thresh)\n",
    "\n",
    "#                 # Update weights\n",
    "#                 lr = optimizer.step_and_update_lr()\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#             if step % log_step == 0:\n",
    "#                 losses = [l.item() for l in losses]\n",
    "#                 message1 = \"Step {}/{}, \".format(step, total_step)\n",
    "#                 message2 = \"Total Loss: {:.4f}, Mel Loss: {:.4f}, Adv Loss: {:.4f}, Pitch Loss: {:.4f}, Energy Loss: {:.4f}, Duration Loss: {:.4f}\".format(\n",
    "#                     *losses\n",
    "#                 )\n",
    "\n",
    "#                 with open(os.path.join(train_log_path, \"log.txt\"), \"a\") as f:\n",
    "#                     f.write(message1 + message2 + \"\\n\")\n",
    "\n",
    "#                 outer_bar.write(message1 + message2)\n",
    "\n",
    "#                 log(train_logger, step, losses=losses, lr=lr, lambdas=lambdas)\n",
    "\n",
    "#             if step % synth_step == 0:\n",
    "#                 fig, wav_reconstruction, wav_prediction, tag = synth_one_sample(\n",
    "#                     batch,\n",
    "#                     output,\n",
    "#                     vocoder,\n",
    "#                     model_config,\n",
    "#                     preprocess_config,\n",
    "#                 )\n",
    "#                 log(\n",
    "#                     train_logger,\n",
    "#                     fig=fig,\n",
    "#                     tag=\"Training/step_{}_{}\".format(step, tag),\n",
    "#                 )\n",
    "#                 sampling_rate = preprocess_config[\"preprocessing\"][\"audio\"][\n",
    "#                     \"sampling_rate\"\n",
    "#                 ]\n",
    "#                 log(\n",
    "#                     train_logger,\n",
    "#                     audio=wav_reconstruction,\n",
    "#                     sampling_rate=sampling_rate,\n",
    "#                     tag=\"Training/step_{}_{}_reconstructed\".format(step, tag),\n",
    "#                 )\n",
    "#                 log(\n",
    "#                     train_logger,\n",
    "#                     audio=wav_prediction,\n",
    "#                     sampling_rate=sampling_rate,\n",
    "#                     tag=\"Training/step_{}_{}_synthesized\".format(step, tag),\n",
    "#                 )\n",
    "\n",
    "#             if step % val_step == 0:\n",
    "#                 model.eval()\n",
    "#                 message = evaluate(model, step, configs, val_logger, vocoder, len(losses), named_param)\n",
    "#                 with open(os.path.join(val_log_path, \"log.txt\"), \"a\") as f:\n",
    "#                     f.write(message + \"\\n\")\n",
    "#                 outer_bar.write(message)\n",
    "\n",
    "#                 model.train()\n",
    "\n",
    "#             if step % save_step == 0:\n",
    "#                 torch.save(\n",
    "#                     {\n",
    "#                         \"model\": model.module.state_dict(),\n",
    "#                         \"optimizer\": optimizer._optimizer.state_dict(),\n",
    "#                     },\n",
    "#                     os.path.join(\n",
    "#                         train_config[\"path\"][\"ckpt_path\"],\n",
    "#                         \"{}.pth.tar\".format(step),\n",
    "#                     ),\n",
    "#                 )\n",
    "\n",
    "#             if step == total_step:\n",
    "#                 quit()\n",
    "#             step += 1\n",
    "#             outer_bar.update(1)\n",
    "\n",
    "#         inner_bar.update(1)\n",
    "#     epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07d408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17b18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs2-env",
   "language": "python",
   "name": "fs2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
