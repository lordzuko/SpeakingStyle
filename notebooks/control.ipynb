{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9f6b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from string import punctuation\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from g2p_en import G2p\n",
    "from pypinyin import pinyin, Style\n",
    "\n",
    "from utils.model import get_vocoder\n",
    "from utils.tools import to_device, synth_samples\n",
    "from dataset import TextDataset\n",
    "from text import text_to_sequence\n",
    "\n",
    "import os\n",
    "from model import ScheduledOptim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4eaa0b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea1e96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lexicon(lex_path):\n",
    "    lexicon = {}\n",
    "    with open(lex_path) as f:\n",
    "        for line in f:\n",
    "            temp = re.split(r\"\\s+\", line.strip(\"\\n\"))\n",
    "            word = temp[0]\n",
    "            phones = temp[1:]\n",
    "            if word.lower() not in lexicon:\n",
    "                lexicon[word.lower()] = phones\n",
    "    return lexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7196412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_english(text, preprocess_config):\n",
    "    text = text.rstrip(punctuation)\n",
    "    lexicon = read_lexicon(preprocess_config[\"path\"][\"lexicon_path\"])\n",
    "\n",
    "    g2p = G2p()\n",
    "    phones = []\n",
    "    words = re.split(r\"([,;.\\-\\?\\!\\s+])\", text)\n",
    "    idx = []\n",
    "    for w in words:\n",
    "        len_before = len(phones)\n",
    "        if w.lower() in lexicon:\n",
    "            phones += lexicon[w.lower()]\n",
    "        else:\n",
    "            phones += list(filter(lambda p: p != \" \", g2p(w)))\n",
    "        if w != \" \":\n",
    "            c_new_phones = len(phones) - len_before\n",
    "            idx.append(c_new_phones)\n",
    "            \n",
    "    phones = \"{\" + \"}{\".join(phones) + \"}\"\n",
    "    phones = re.sub(r\"\\{[^\\w\\s]?\\}\", \"{sp}\", phones)\n",
    "    phones = phones.replace(\"}{\", \" \")\n",
    "    words = [w for w in words if w != \" \"]\n",
    "    print(\"Raw Text Sequence: {}\".format(text))\n",
    "    print(\"Phoneme Sequence: {}\".format(phones))\n",
    "    print(\"Words: {}\".format(words))\n",
    "    print(\"Idx: {}\".format(idx))\n",
    "    sequence = np.array(\n",
    "        text_to_sequence(\n",
    "            phones, preprocess_config[\"preprocessing\"][\"text\"][\"text_cleaners\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return np.array(sequence), words, idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24e67177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize(model, step, configs, vocoder, batchs, control_values):\n",
    "    preprocess_config, model_config, train_config = configs\n",
    "    pitch_control, energy_control, duration_control = control_values\n",
    "\n",
    "    for batch in tqdm(batchs, total=len(batchs), desc=\"batches:> \"):\n",
    "        batch = to_device(batch, device)\n",
    "        with torch.no_grad():\n",
    "            # Forward\n",
    "            output = model(\n",
    "                *(batch[2:]),\n",
    "                p_control=pitch_control,\n",
    "                e_control=energy_control,\n",
    "                d_control=duration_control\n",
    "            )\n",
    "            synth_samples(\n",
    "                batch,\n",
    "                output,\n",
    "                vocoder,\n",
    "                model_config,\n",
    "                preprocess_config,\n",
    "                train_config[\"path\"][\"result_path\"],\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "db976703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.tools import get_mask_from_lengths, pad\n",
    "\n",
    "\n",
    "\n",
    "class VarianceAdaptor(nn.Module):\n",
    "    \"\"\"Variance Adaptor\"\"\"\n",
    "\n",
    "    def __init__(self, preprocess_config, model_config):\n",
    "        super(VarianceAdaptor, self).__init__()\n",
    "        self.duration_predictor = VariancePredictor(model_config)\n",
    "        self.length_regulator = LengthRegulator()\n",
    "        self.pitch_predictor = VariancePredictor(model_config)\n",
    "        self.energy_predictor = VariancePredictor(model_config)\n",
    "\n",
    "        self.pitch_feature_level = preprocess_config[\"preprocessing\"][\"pitch\"][\n",
    "            \"feature\"\n",
    "        ]\n",
    "        self.energy_feature_level = preprocess_config[\"preprocessing\"][\"energy\"][\n",
    "            \"feature\"\n",
    "        ]\n",
    "        assert self.pitch_feature_level in [\"phoneme_level\", \"frame_level\"]\n",
    "        assert self.energy_feature_level in [\"phoneme_level\", \"frame_level\"]\n",
    "\n",
    "        pitch_quantization = model_config[\"variance_embedding\"][\"pitch_quantization\"]\n",
    "        energy_quantization = model_config[\"variance_embedding\"][\"energy_quantization\"]\n",
    "        n_bins = model_config[\"variance_embedding\"][\"n_bins\"]\n",
    "        assert pitch_quantization in [\"linear\", \"log\"]\n",
    "        assert energy_quantization in [\"linear\", \"log\"]\n",
    "        with open(\n",
    "            os.path.join(preprocess_config[\"path\"][\"preprocessed_path\"], \"stats.json\")\n",
    "        ) as f:\n",
    "            stats = json.load(f)\n",
    "            pitch_min, pitch_max = stats[\"pitch\"][:2]\n",
    "            energy_min, energy_max = stats[\"energy\"][:2]\n",
    "\n",
    "        if pitch_quantization == \"log\":\n",
    "            self.pitch_bins = nn.Parameter(\n",
    "                torch.exp(\n",
    "                    torch.linspace(np.log(pitch_min), np.log(pitch_max), n_bins - 1)\n",
    "                ),\n",
    "                requires_grad=False,\n",
    "            )\n",
    "        else:\n",
    "            self.pitch_bins = nn.Parameter(\n",
    "                torch.linspace(pitch_min, pitch_max, n_bins - 1),\n",
    "                requires_grad=False,\n",
    "            )\n",
    "        if energy_quantization == \"log\":\n",
    "            self.energy_bins = nn.Parameter(\n",
    "                torch.exp(\n",
    "                    torch.linspace(np.log(energy_min), np.log(energy_max), n_bins - 1)\n",
    "                ),\n",
    "                requires_grad=False,\n",
    "            )\n",
    "        else:\n",
    "            self.energy_bins = nn.Parameter(\n",
    "                torch.linspace(energy_min, energy_max, n_bins - 1),\n",
    "                requires_grad=False,\n",
    "            )\n",
    "\n",
    "        self.pitch_embedding = nn.Embedding(\n",
    "            n_bins, model_config[\"transformer\"][\"encoder_hidden\"]\n",
    "        )\n",
    "        self.energy_embedding = nn.Embedding(\n",
    "            n_bins, model_config[\"transformer\"][\"encoder_hidden\"]\n",
    "        )\n",
    "\n",
    "    def get_pitch_embedding(self, x, target, mask, control):\n",
    "        prediction = self.pitch_predictor(x, mask)\n",
    "        \n",
    "        if target is not None:\n",
    "            embedding = self.pitch_embedding(torch.bucketize(target, self.pitch_bins))\n",
    "        else:\n",
    "            prediction = prediction * control\n",
    "            embedding = self.pitch_embedding(\n",
    "                torch.bucketize(prediction, self.pitch_bins)\n",
    "            )\n",
    "        print(f\"Pitch: {prediction} \\n{prediction.shape}\")\n",
    "        return prediction, embedding\n",
    "\n",
    "    def get_energy_embedding(self, x, target, mask, control):\n",
    "        prediction = self.energy_predictor(x, mask)\n",
    "        \n",
    "        if target is not None:\n",
    "            embedding = self.energy_embedding(torch.bucketize(target, self.energy_bins))\n",
    "        else:\n",
    "            prediction = prediction * control\n",
    "            embedding = self.energy_embedding(\n",
    "                torch.bucketize(prediction, self.energy_bins)\n",
    "            )\n",
    "        print(f\"Energy: {prediction} \\n{prediction.shape}\")\n",
    "        return prediction, embedding\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        src_mask,\n",
    "        mel_mask=None,\n",
    "        max_len=None,\n",
    "        pitch_target=None,\n",
    "        energy_target=None,\n",
    "        duration_target=None,\n",
    "        p_control=1.0,\n",
    "        e_control=1.0,\n",
    "        d_control=1.0,\n",
    "    ):\n",
    "\n",
    "        log_duration_prediction = self.duration_predictor(x, src_mask)\n",
    "        if self.pitch_feature_level == \"phoneme_level\":\n",
    "            pitch_prediction, pitch_embedding = self.get_pitch_embedding(\n",
    "                x, pitch_target, src_mask, p_control\n",
    "            )\n",
    "            x = x + pitch_embedding\n",
    "        if self.energy_feature_level == \"phoneme_level\":\n",
    "            energy_prediction, energy_embedding = self.get_energy_embedding(\n",
    "                x, energy_target, src_mask, e_control\n",
    "            )\n",
    "            x = x + energy_embedding\n",
    "\n",
    "        if duration_target is not None:\n",
    "            x, mel_len = self.length_regulator(x, duration_target, max_len)\n",
    "            duration_rounded = duration_target\n",
    "        else:\n",
    "            duration_rounded = torch.clamp(\n",
    "                (torch.round(torch.exp(log_duration_prediction) - 1) * d_control),\n",
    "                min=0,\n",
    "            )\n",
    "            print(f\"Duration: {duration_rounded}\")\n",
    "            x, mel_len = self.length_regulator(x, duration_rounded, max_len)\n",
    "            mel_mask = get_mask_from_lengths(mel_len)\n",
    "\n",
    "        if self.pitch_feature_level == \"frame_level\":\n",
    "            pitch_prediction, pitch_embedding = self.get_pitch_embedding(\n",
    "                x, pitch_target, mel_mask, p_control\n",
    "            )\n",
    "            x = x + pitch_embedding\n",
    "        if self.energy_feature_level == \"frame_level\":\n",
    "            energy_prediction, energy_embedding = self.get_energy_embedding(\n",
    "                x, energy_target, mel_mask, e_control\n",
    "            )\n",
    "            x = x + energy_embedding\n",
    "\n",
    "        return (\n",
    "            x,\n",
    "            pitch_prediction,\n",
    "            energy_prediction,\n",
    "            log_duration_prediction,\n",
    "            duration_rounded,\n",
    "            mel_len,\n",
    "            mel_mask,\n",
    "        )\n",
    "\n",
    "\n",
    "class LengthRegulator(nn.Module):\n",
    "    \"\"\"Length Regulator\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LengthRegulator, self).__init__()\n",
    "\n",
    "    def LR(self, x, duration, max_len):\n",
    "        output = list()\n",
    "        mel_len = list()\n",
    "        for batch, expand_target in zip(x, duration):\n",
    "            expanded = self.expand(batch, expand_target)\n",
    "            output.append(expanded)\n",
    "            mel_len.append(expanded.shape[0])\n",
    "\n",
    "        if max_len is not None:\n",
    "            output = pad(output, max_len)\n",
    "        else:\n",
    "            output = pad(output)\n",
    "\n",
    "        return output, torch.LongTensor(mel_len).to(device)\n",
    "\n",
    "    def expand(self, batch, predicted):\n",
    "        out = list()\n",
    "\n",
    "        for i, vec in enumerate(batch):\n",
    "            expand_size = predicted[i].item()\n",
    "            out.append(vec.expand(max(int(expand_size), 0), -1))\n",
    "        out = torch.cat(out, 0)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, duration, max_len):\n",
    "        output, mel_len = self.LR(x, duration, max_len)\n",
    "        return output, mel_len\n",
    "\n",
    "\n",
    "class VariancePredictor(nn.Module):\n",
    "    \"\"\"Duration, Pitch and Energy Predictor\"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "        super(VariancePredictor, self).__init__()\n",
    "\n",
    "        self.input_size = model_config[\"transformer\"][\"encoder_hidden\"]\n",
    "        self.filter_size = model_config[\"variance_predictor\"][\"filter_size\"]\n",
    "        self.kernel = model_config[\"variance_predictor\"][\"kernel_size\"]\n",
    "        self.conv_output_size = model_config[\"variance_predictor\"][\"filter_size\"]\n",
    "        self.dropout = model_config[\"variance_predictor\"][\"dropout\"]\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        \"conv1d_1\",\n",
    "                        Conv(\n",
    "                            self.input_size,\n",
    "                            self.filter_size,\n",
    "                            kernel_size=self.kernel,\n",
    "                            padding=(self.kernel - 1) // 2,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (\"relu_1\", nn.ReLU()),\n",
    "                    (\"layer_norm_1\", nn.LayerNorm(self.filter_size)),\n",
    "                    (\"dropout_1\", nn.Dropout(self.dropout)),\n",
    "                    (\n",
    "                        \"conv1d_2\",\n",
    "                        Conv(\n",
    "                            self.filter_size,\n",
    "                            self.filter_size,\n",
    "                            kernel_size=self.kernel,\n",
    "                            padding=1,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (\"relu_2\", nn.ReLU()),\n",
    "                    (\"layer_norm_2\", nn.LayerNorm(self.filter_size)),\n",
    "                    (\"dropout_2\", nn.Dropout(self.dropout)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.linear_layer = nn.Linear(self.conv_output_size, 1)\n",
    "\n",
    "    def forward(self, encoder_output, mask):\n",
    "        out = self.conv_layer(encoder_output)\n",
    "        out = self.linear_layer(out)\n",
    "        out = out.squeeze(-1)\n",
    "\n",
    "        if mask is not None:\n",
    "            out = out.masked_fill(mask, 0.0)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=1,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1,\n",
    "        bias=True,\n",
    "        w_init=\"linear\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param in_channels: dimension of input\n",
    "        :param out_channels: dimension of output\n",
    "        :param kernel_size: size of kernel\n",
    "        :param stride: size of stride\n",
    "        :param padding: size of padding\n",
    "        :param dilation: dilation rate\n",
    "        :param bias: boolean. if True, bias is included.\n",
    "        :param w_init: str. weight inits with xavier initialization.\n",
    "        \"\"\"\n",
    "        super(Conv, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().transpose(1, 2)\n",
    "        x = self.conv(x)\n",
    "        x = x.contiguous().transpose(1, 2)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0576f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformer import Encoder, Decoder, PostNet\n",
    "from utils.tools import get_mask_from_lengths\n",
    "\n",
    "\n",
    "class FastSpeech2(nn.Module):\n",
    "    \"\"\" FastSpeech2 \"\"\"\n",
    "\n",
    "    def __init__(self, preprocess_config, model_config):\n",
    "        super(FastSpeech2, self).__init__()\n",
    "        self.model_config = model_config\n",
    "\n",
    "        self.encoder = Encoder(model_config)\n",
    "        self.variance_adaptor = VarianceAdaptor(preprocess_config, model_config)\n",
    "        self.decoder = Decoder(model_config)\n",
    "        self.mel_linear = nn.Linear(\n",
    "            model_config[\"transformer\"][\"decoder_hidden\"],\n",
    "            preprocess_config[\"preprocessing\"][\"mel\"][\"n_mel_channels\"],\n",
    "        )\n",
    "        self.postnet = PostNet()\n",
    "\n",
    "        self.speaker_emb = None\n",
    "        if model_config[\"multi_speaker\"]:\n",
    "            with open(\n",
    "                os.path.join(\n",
    "                    preprocess_config[\"path\"][\"preprocessed_path\"], \"speakers.json\"\n",
    "                ),\n",
    "                \"r\",\n",
    "            ) as f:\n",
    "                n_speaker = len(json.load(f))\n",
    "            self.speaker_emb = nn.Embedding(\n",
    "                n_speaker,\n",
    "                model_config[\"transformer\"][\"encoder_hidden\"],\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        speakers,\n",
    "        texts,\n",
    "        src_lens,\n",
    "        max_src_len,\n",
    "        mels=None,\n",
    "        mel_lens=None,\n",
    "        max_mel_len=None,\n",
    "        p_targets=None,\n",
    "        e_targets=None,\n",
    "        d_targets=None,\n",
    "        p_control=1.0,\n",
    "        e_control=1.0,\n",
    "        d_control=1.0,\n",
    "    ):\n",
    "        src_masks = get_mask_from_lengths(src_lens, max_src_len)\n",
    "        mel_masks = (\n",
    "            get_mask_from_lengths(mel_lens, max_mel_len)\n",
    "            if mel_lens is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        output = self.encoder(texts, src_masks)\n",
    "\n",
    "        if self.speaker_emb is not None:\n",
    "            output = output + self.speaker_emb(speakers).unsqueeze(1).expand(\n",
    "                -1, max_src_len, -1\n",
    "            )\n",
    "\n",
    "        (\n",
    "            output,\n",
    "            p_predictions,\n",
    "            e_predictions,\n",
    "            log_d_predictions,\n",
    "            d_rounded,\n",
    "            mel_lens,\n",
    "            mel_masks,\n",
    "        ) = self.variance_adaptor(\n",
    "            output,\n",
    "            src_masks,\n",
    "            mel_masks,\n",
    "            max_mel_len,\n",
    "            p_targets,\n",
    "            e_targets,\n",
    "            d_targets,\n",
    "            p_control,\n",
    "            e_control,\n",
    "            d_control,\n",
    "        )\n",
    "\n",
    "        output, mel_masks = self.decoder(output, mel_masks)\n",
    "        output = self.mel_linear(output)\n",
    "\n",
    "        postnet_output = self.postnet(output) + output\n",
    "\n",
    "        return (\n",
    "            output,\n",
    "            postnet_output,\n",
    "            p_predictions,\n",
    "            e_predictions,\n",
    "            log_d_predictions,\n",
    "            d_rounded,\n",
    "            src_masks,\n",
    "            mel_masks,\n",
    "            src_lens,\n",
    "            mel_lens,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e096af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5fcd27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configs\n",
    "\n",
    "PREPROCESS_CONFIG_PATH = \"/work/tc046/tc046/lordzuko/work/SpeakingStyle/config/BC2013/preprocess.yaml\"\n",
    "MODEL_CONFIG_PATH = \"/work/tc046/tc046/lordzuko/work/SpeakingStyle/config/BC2013/model.yaml\"\n",
    "TRAIN_CONFIG_PATH = \"/work/tc046/tc046/lordzuko/work/SpeakingStyle/config/BC2013/train.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "15e2cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_config = yaml.load(\n",
    "    open(PREPROCESS_CONFIG_PATH, \"r\"), Loader=yaml.FullLoader\n",
    ")\n",
    "model_config = yaml.load(open(MODEL_CONFIG_PATH, \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(TRAIN_CONFIG_PATH, \"r\"), Loader=yaml.FullLoader)\n",
    "configs = (preprocess_config, model_config, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0bc80569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args, configs, device, train=False):\n",
    "    (preprocess_config, model_config, train_config) = configs\n",
    "\n",
    "    model = FastSpeech2(preprocess_config, model_config).to(device)\n",
    "    if args[\"restore_step\"]:\n",
    "        ckpt_path = os.path.join(\n",
    "            train_config[\"path\"][\"ckpt_path\"],\n",
    "            \"{}.pth.tar\".format(args[\"restore_step\"]),\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            ckpt = torch.load(ckpt_path)\n",
    "        else:\n",
    "            ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "    if train:\n",
    "        scheduled_optim = ScheduledOptim(\n",
    "            model, train_config, model_config, args[\"restore_step\"]\n",
    "        )\n",
    "        if args[\"restore_step\"]:\n",
    "            scheduled_optim.load_state_dict(ckpt[\"optimizer\"])\n",
    "        model.train()\n",
    "        return model, scheduled_optim\n",
    "\n",
    "    model.eval()\n",
    "    model.requires_grad_ = False\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "add863c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"restore_step\"] = 61000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e917091d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "Model Loaded\n",
      "Loading Vocoder...\n",
      "Removing weight norm...\n",
      "Vocoder Loaded\n"
     ]
    }
   ],
   "source": [
    "# Get model\n",
    "print(\"Loading Model...\")\n",
    "model = get_model(args, configs, device, train=False)\n",
    "print(\"Model Loaded\")\n",
    "# Load vocoder\n",
    "print(\"Loading Vocoder...\")\n",
    "vocoder = get_vocoder(model_config, device)\n",
    "print(\"Vocoder Loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3c009cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_speech(text, speaker_id=0, pitch_control=1.0, energy_control=1.0, duration_control=1.0, fine_control={}):\n",
    "    ids = raw_texts = [text[:100]]\n",
    "    speakers = np.array([speaker_id])\n",
    "    if preprocess_config[\"preprocessing\"][\"text\"][\"language\"] == \"en\":\n",
    "        texts = np.array([preprocess_english(text, preprocess_config)])\n",
    "    elif preprocess_config[\"preprocessing\"][\"text\"][\"language\"] == \"zh\":\n",
    "        texts = np.array([preprocess_mandarin(text, preprocess_config)])\n",
    "    text_lens = np.array([len(texts[0])])\n",
    "    batchs = [(ids, raw_texts, speakers, texts, text_lens, max(text_lens))]\n",
    "            \n",
    "    control_values = pitch_control, energy_control, duration_control        \n",
    "    print(\"Synthesizing ...\")\n",
    "    synthesize(model, args[\"restore_step\"], configs, vocoder, batchs, control_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7889d1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text Sequence: synthesis\n",
      "Phoneme Sequence: {S IH1 N TH AH0 S AH0 S}\n",
      "Words: ['synthesis']\n",
      "Idx: [8]\n",
      "Synthesizing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "batches:> :   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch: tensor([[ 0.1739,  0.8873,  0.6487,  0.2073,  0.1662, -0.3408, -0.4615, -0.7759]],\n",
      "       device='cuda:0') \n",
      "torch.Size([1, 8])\n",
      "Energy: tensor([[-0.1701,  3.5771,  3.3476,  0.1656,  0.6157, -0.1228,  0.2820, -0.4996]],\n",
      "       device='cuda:0') \n",
      "torch.Size([1, 8])\n",
      "Duration: tensor([[14.,  6.,  6.,  5.,  4.,  8.,  6., 11.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches:> : 100%|██████████| 1/1 [00:08<00:00,  8.54s/it]\n"
     ]
    }
   ],
   "source": [
    "synth_speech(text=\"synthesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0af44931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text Sequence: synthesis is cool\n",
      "Phoneme Sequence: {S IH1 N TH AH0 S AH0 S IH0 Z K UW1 L}\n",
      "Words: ['synthesis', 'is', 'cool']\n",
      "Idx: [8, 2, 3]\n",
      "Synthesizing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches:> : 100%|██████████| 1/1 [00:01<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch: tensor([[ 0.1186,  1.1969,  1.3294,  1.1833,  0.5511, -0.2413, -0.5019, -0.7311,\n",
      "         -0.2597, -0.6401, -0.3367,  0.4571, -0.7197]], device='cuda:0') \n",
      "torch.Size([1, 13])\n",
      "Energy: tensor([[-0.4386,  2.7543,  3.3090, -0.2172,  0.5745, -0.5562, -0.0577, -0.5846,\n",
      "         -0.1813, -0.4881, -1.1717, -0.0766, -0.1817]], device='cuda:0') \n",
      "torch.Size([1, 13])\n",
      "Duration: tensor([[17.,  7.,  7.,  5.,  6.,  9.,  5.,  8.,  4.,  8., 11., 18.,  8.]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "synth_speech(text=\"synthesis is cool\", pitch_control=1.0, energy_control=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3b9daa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text Sequence: synthesis is cool\n",
      "Phoneme Sequence: {S IH1 N TH AH0 S AH0 S IH0 Z K UW1 L}\n",
      "Words: ['synthesis', 'is', 'cool']\n",
      "Idx: [8, 2, 3]\n",
      "Synthesizing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "batches:> :   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch: tensor([[ 0.0593,  0.5985,  0.6647,  0.5917,  0.2756, -0.1206, -0.2509, -0.3655,\n",
      "         -0.1298, -0.3200, -0.1683,  0.2285, -0.3598]], device='cuda:0') \n",
      "torch.Size([1, 13])\n",
      "Energy: tensor([[-2.5605e-01,  1.2461e+00,  1.3168e+00, -1.6128e-01,  2.3102e-01,\n",
      "         -2.8790e-01, -9.2617e-04, -2.5511e-01, -8.9651e-02, -2.1404e-01,\n",
      "         -5.9024e-01, -7.2405e-02, -2.3174e-02]], device='cuda:0') \n",
      "torch.Size([1, 13])\n",
      "Duration: tensor([[17.,  7.,  7.,  5.,  6.,  9.,  5.,  8.,  4.,  8., 11., 18.,  8.]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches:> : 100%|██████████| 1/1 [00:03<00:00,  3.19s/it]\n"
     ]
    }
   ],
   "source": [
    "synth_speech(text=\"synthesis is cool\", pitch_control=1.0, energy_control=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f419bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlledVarianceAdapter(VarianceAdaptor):\n",
    "    \n",
    "    def __init__(self, preprocess_config, model_config):\n",
    "        super(ControlledVarianceAdapter, self).__init__(preprocess_config, model_config)\n",
    "        \n",
    "    def get_pitch_embedding(self, x, target, mask, control):\n",
    "        prediction = self.pitch_predictor(x, mask)\n",
    "        \n",
    "        if target is not None:\n",
    "            embedding = self.pitch_embedding(torch.bucketize(target, self.pitch_bins))\n",
    "        else:\n",
    "            if isinstance(control, float):\n",
    "                prediction = prediction * control\n",
    "            elif isinstance(control, list):\n",
    "                prediction = prediction * torch.from_numpy(np.array(control)).to(device)\n",
    "            embedding = self.pitch_embedding(\n",
    "                torch.bucketize(prediction, self.pitch_bins)\n",
    "            )\n",
    "        print(f\"Pitch: {prediction} \\n{prediction.shape}\")\n",
    "        return prediction, embedding\n",
    "\n",
    "    def get_energy_embedding(self, x, target, mask, control):\n",
    "        prediction = self.energy_predictor(x, mask)\n",
    "        \n",
    "        if target is not None:\n",
    "            embedding = self.energy_embedding(torch.bucketize(target, self.energy_bins))\n",
    "        else:\n",
    "            if isinstance(control, float):\n",
    "                prediction = prediction * control\n",
    "            elif isinstance(control, list):\n",
    "                prediction = prediction * torch.from_numpy(np.array(control)).to(device)\n",
    "            embedding = self.energy_embedding(\n",
    "                torch.bucketize(prediction, self.energy_bins)\n",
    "            )\n",
    "        print(f\"Energy: {prediction} \\n{prediction.shape}\")\n",
    "        return prediction, embedding\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        src_mask,\n",
    "        mel_mask=None,\n",
    "        max_len=None,\n",
    "        pitch_target=None,\n",
    "        energy_target=None,\n",
    "        duration_target=None,\n",
    "        p_control=1.0,\n",
    "        e_control=1.0,\n",
    "        d_control=1.0,\n",
    "    ):\n",
    "\n",
    "        log_duration_prediction = self.duration_predictor(x, src_mask)\n",
    "        if self.pitch_feature_level == \"phoneme_level\":\n",
    "            pitch_prediction, pitch_embedding = self.get_pitch_embedding(\n",
    "                x, pitch_target, src_mask, p_control\n",
    "            )\n",
    "            x = x + pitch_embedding\n",
    "        if self.energy_feature_level == \"phoneme_level\":\n",
    "            energy_prediction, energy_embedding = self.get_energy_embedding(\n",
    "                x, energy_target, src_mask, e_control\n",
    "            )\n",
    "            x = x + energy_embedding\n",
    "\n",
    "        if duration_target is not None:\n",
    "            x, mel_len = self.length_regulator(x, duration_target, max_len)\n",
    "            duration_rounded = duration_target\n",
    "        else:\n",
    "            if isinstance(d_control, float):\n",
    "                duration_rounded = torch.clamp(\n",
    "                    (torch.round(torch.exp(log_duration_prediction) - 1) * d_control),\n",
    "                    min=0,\n",
    "                )\n",
    "            elif isinstance(d_control, list):\n",
    "                duration_rounded = torch.clamp(\n",
    "                    (torch.round(torch.exp(log_duration_prediction) - 1) * torch.from_numpy(np.array(d_control)).to(device)),\n",
    "                    min=0,\n",
    "                )\n",
    "            print(f\"Duration: {duration_rounded}\")\n",
    "            x, mel_len = self.length_regulator(x, duration_rounded, max_len)\n",
    "            mel_mask = get_mask_from_lengths(mel_len)\n",
    "\n",
    "        if self.pitch_feature_level == \"frame_level\":\n",
    "            pitch_prediction, pitch_embedding = self.get_pitch_embedding(\n",
    "                x, pitch_target, mel_mask, p_control\n",
    "            )\n",
    "            x = x + pitch_embedding\n",
    "        if self.energy_feature_level == \"frame_level\":\n",
    "            energy_prediction, energy_embedding = self.get_energy_embedding(\n",
    "                x, energy_target, mel_mask, e_control\n",
    "            )\n",
    "            x = x + energy_embedding\n",
    "\n",
    "        return (\n",
    "            x,\n",
    "            pitch_prediction,\n",
    "            energy_prediction,\n",
    "            log_duration_prediction,\n",
    "            duration_rounded,\n",
    "            mel_len,\n",
    "            mel_mask,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "334e8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformer import Encoder, Decoder, PostNet\n",
    "from utils.tools import get_mask_from_lengths\n",
    "\n",
    "\n",
    "class FastSpeech2(nn.Module):\n",
    "    \"\"\" FastSpeech2 \"\"\"\n",
    "\n",
    "    def __init__(self, preprocess_config, model_config):\n",
    "        super(FastSpeech2, self).__init__()\n",
    "        self.model_config = model_config\n",
    "\n",
    "        self.encoder = Encoder(model_config)\n",
    "        self.variance_adaptor = ControlledVarianceAdapter(preprocess_config, model_config)\n",
    "        self.decoder = Decoder(model_config)\n",
    "        self.mel_linear = nn.Linear(\n",
    "            model_config[\"transformer\"][\"decoder_hidden\"],\n",
    "            preprocess_config[\"preprocessing\"][\"mel\"][\"n_mel_channels\"],\n",
    "        )\n",
    "        self.postnet = PostNet()\n",
    "\n",
    "        self.speaker_emb = None\n",
    "        if model_config[\"multi_speaker\"]:\n",
    "            with open(\n",
    "                os.path.join(\n",
    "                    preprocess_config[\"path\"][\"preprocessed_path\"], \"speakers.json\"\n",
    "                ),\n",
    "                \"r\",\n",
    "            ) as f:\n",
    "                n_speaker = len(json.load(f))\n",
    "            self.speaker_emb = nn.Embedding(\n",
    "                n_speaker,\n",
    "                model_config[\"transformer\"][\"encoder_hidden\"],\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        speakers,\n",
    "        texts,\n",
    "        src_lens,\n",
    "        max_src_len,\n",
    "        mels=None,\n",
    "        mel_lens=None,\n",
    "        max_mel_len=None,\n",
    "        p_targets=None,\n",
    "        e_targets=None,\n",
    "        d_targets=None,\n",
    "        p_control=1.0,\n",
    "        e_control=1.0,\n",
    "        d_control=1.0,\n",
    "    ):\n",
    "        src_masks = get_mask_from_lengths(src_lens, max_src_len)\n",
    "        mel_masks = (\n",
    "            get_mask_from_lengths(mel_lens, max_mel_len)\n",
    "            if mel_lens is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        output = self.encoder(texts, src_masks)\n",
    "\n",
    "        if self.speaker_emb is not None:\n",
    "            output = output + self.speaker_emb(speakers).unsqueeze(1).expand(\n",
    "                -1, max_src_len, -1\n",
    "            )\n",
    "\n",
    "        (\n",
    "            output,\n",
    "            p_predictions,\n",
    "            e_predictions,\n",
    "            log_d_predictions,\n",
    "            d_rounded,\n",
    "            mel_lens,\n",
    "            mel_masks,\n",
    "        ) = self.variance_adaptor(\n",
    "            output,\n",
    "            src_masks,\n",
    "            mel_masks,\n",
    "            max_mel_len,\n",
    "            p_targets,\n",
    "            e_targets,\n",
    "            d_targets,\n",
    "            p_control,\n",
    "            e_control,\n",
    "            d_control,\n",
    "        )\n",
    "\n",
    "        output, mel_masks = self.decoder(output, mel_masks)\n",
    "        output = self.mel_linear(output)\n",
    "\n",
    "        postnet_output = self.postnet(output) + output\n",
    "\n",
    "        return (\n",
    "            output,\n",
    "            postnet_output,\n",
    "            p_predictions,\n",
    "            e_predictions,\n",
    "            log_d_predictions,\n",
    "            d_rounded,\n",
    "            src_masks,\n",
    "            mel_masks,\n",
    "            src_lens,\n",
    "            mel_lens,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d8bab05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model2(args, configs, device, train=False):\n",
    "    (preprocess_config, model_config, train_config) = configs\n",
    "\n",
    "    model = FastSpeech2(preprocess_config, model_config).to(device)\n",
    "    if args[\"restore_step\"]:\n",
    "        ckpt_path = os.path.join(\n",
    "            train_config[\"path\"][\"ckpt_path\"],\n",
    "            \"{}.pth.tar\".format(args[\"restore_step\"]),\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            ckpt = torch.load(ckpt_path)\n",
    "        else:\n",
    "            ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "    if train:\n",
    "        scheduled_optim = ScheduledOptim(\n",
    "            model, train_config, model_config, args[\"restore_step\"]\n",
    "        )\n",
    "        if args[\"restore_step\"]:\n",
    "            scheduled_optim.load_state_dict(ckpt[\"optimizer\"])\n",
    "        model.train()\n",
    "        return model, scheduled_optim\n",
    "\n",
    "    model.eval()\n",
    "    model.requires_grad_ = False\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "952791e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_speech2(text, speaker_id=0, pitch_control=1.0, energy_control=1.0, duration_control=1.0, fine_control={}):\n",
    "    ids = raw_texts = [text[:100]]\n",
    "    speakers = np.array([speaker_id])\n",
    "    if preprocess_config[\"preprocessing\"][\"text\"][\"language\"] == \"en\":\n",
    "        out = preprocess_english(text, preprocess_config)\n",
    "        texts, words, idxs = np.array([out[0]]), out[1], out[2]\n",
    "    elif preprocess_config[\"preprocessing\"][\"text\"][\"language\"] == \"zh\":\n",
    "        texts = np.array([preprocess_mandarin(text, preprocess_config)])\n",
    "    text_lens = np.array([len(texts[0])])\n",
    "    batchs = [(ids, raw_texts, speakers, texts, text_lens, max(text_lens))]\n",
    "    print(batchs)\n",
    "    print(words)\n",
    "    print(idxs)\n",
    "    if fine_control:\n",
    "        energy_control = []\n",
    "        duration_control = []\n",
    "        pitch_control = []\n",
    "        for i, x in enumerate(idxs):\n",
    "            for _ in range(x):\n",
    "                energy_control.append(fine_control[\"energy\"][0][i])\n",
    "                pitch_control.append(fine_control[\"pitch\"][0][i])\n",
    "                duration_control.append(fine_control[\"duration\"][0][i])\n",
    "        print(energy_control)\n",
    "        print(pitch_control)\n",
    "        print(duration_control)\n",
    "        control_values = pitch_control, energy_control, duration_control\n",
    "    else: \n",
    "        control_values = pitch_control, energy_control, duration_control        \n",
    "    print(\"Synthesizing ...\")\n",
    "    synthesize(model2, args[\"restore_step\"], configs, vocoder, batchs, control_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1908d425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "# Get model\n",
    "print(\"Loading Model...\")\n",
    "model2 = get_model2(args, configs, device, train=False)\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e2e56ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_control = {\n",
    "    \"pitch\": [[0.7, 1, 1.3]],\n",
    "    \"energy\": [[1.1, 1, 0.8]],\n",
    "    \"duration\": [[0.7, 1, 1.5]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "307150f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text Sequence: synthesis is cool\n",
      "Phoneme Sequence: {S IH1 N TH AH0 S AH0 S IH0 Z K UW1 L}\n",
      "Words: ['synthesis', 'is', 'cool']\n",
      "Idx: [8, 2, 3]\n",
      "[(['synthesis is cool'], ['synthesis is cool'], array([0]), array([[131, 109, 119, 134,  73, 131,  73, 131, 108, 146, 116, 141, 117]]), array([13]), 13)]\n",
      "['synthesis', 'is', 'cool']\n",
      "[8, 2, 3]\n",
      "[1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1, 1, 0.8, 0.8, 0.8]\n",
      "[0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 1, 1.3, 1.3, 1.3]\n",
      "[0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 1, 1.5, 1.5, 1.5]\n",
      "Synthesizing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "batches:> :   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch: tensor([[ 0.0830,  0.8379,  0.9305,  0.8283,  0.3858, -0.1689, -0.3513, -0.5117,\n",
      "         -0.2597, -0.6401, -0.4376,  0.5942, -0.9356]], device='cuda:0',\n",
      "       dtype=torch.float64) \n",
      "torch.Size([1, 13])\n",
      "Energy: tensor([[-0.5355,  2.8602,  3.1640, -0.4525,  0.5363, -0.6024, -0.0158, -0.5996,\n",
      "         -0.1858, -0.4825, -0.9323, -0.0455, -0.1929]], device='cuda:0',\n",
      "       dtype=torch.float64) \n",
      "torch.Size([1, 13])\n",
      "Duration: tensor([[11.9000,  4.9000,  4.9000,  3.5000,  4.2000,  6.3000,  3.5000,  5.6000,\n",
      "          4.0000,  8.0000, 16.5000, 27.0000, 12.0000]], device='cuda:0',\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches:> : 100%|██████████| 1/1 [00:07<00:00,  7.78s/it]\n"
     ]
    }
   ],
   "source": [
    "synth_speech2(text=\"synthesis is cool\", fine_control=fine_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a33e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91a5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs2-env",
   "language": "python",
   "name": "fs2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
